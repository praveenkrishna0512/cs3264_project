{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h5T6rJA3w0XL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Flatten,Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import csv\n",
    "import nltk\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yif2s43w-Amb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Input,Concatenate,Dropout,Dense,BatchNormalization,Conv1D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import scipy\n",
    "from tensorflow.keras.initializers import he_normal,glorot_normal\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler,ReduceLROnPlateau\n",
    "from time import time\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input,BatchNormalization,Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import random as rn\n",
    "import string\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.initializers import glorot_uniform,glorot_normal\n",
    "from tensorflow.keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "coVpb7Lg-T1R",
    "outputId": "c3bb1cfa-5ba8-4eb9-bfab-2203324f2369"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading the training data\n",
    "training_data=pd.read_csv(\"./data/train.csv\")\n",
    "display(training_data.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing. \n",
    "\n",
    "Preprocessing includes removing spaces, special characters, contractions and stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gl8fSjkIxPiP",
    "outputId": "a3965b3c-7b32-461c-ccb9-778f582ca8ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3911/3911 [00:00<00:00, 7438.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "def decontractions(phrase):\n",
    "   \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "#preprocessing: replacing special characters and space and make text lowercase\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "stopwords = stopwords.words('english')\n",
    "def preprocess(text_col,stopword):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(text_col.values):\n",
    "        # Replace \"carriage return\" with \"space\".\n",
    "        sentence=str(sentence)\n",
    "        sent = sentence.replace('\\\\r', ' ')\n",
    "        # Replace \"quotes\" with \"space\".\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        # Replace \"line feed\" with \"space\".\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "         # Replace characters between words with \"space\".\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        #remove stop words\n",
    "        #decontraction\n",
    "        sent=decontractions(sent)\n",
    "        if stopword:\n",
    "            sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "        else:\n",
    "            sent = ' '.join(e for e in sent.split())\n",
    "        # to lowercase\n",
    "        preprocessed.append(sent.lower().strip())\n",
    "    return preprocessed\n",
    "training_data['full_text']=preprocess(training_data['full_text'],stopword=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>i think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>when a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>dear principal if u change the school policy o...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>the best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  dear principal if u change the school policy o...       3.0   \n",
       "3  003885A45F42  the best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  \n",
       "2     3.5         3.0          3.0      3.0          2.5  \n",
       "3     4.5         4.5          4.5      4.0          5.0  \n",
       "4     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-17b3TaLxxTF",
    "outputId": "b72ca6a1-c6a8-42b5-fbd0-fc2d4a5737ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 1) (3128, 6)\n"
     ]
    }
   ],
   "source": [
    "#Train -test split, 20% of the data for validation set.\n",
    "y=training_data[['cohesion','syntax','vocabulary','phraseology','grammar','conventions']]\n",
    "X=training_data.drop(['text_id','cohesion','syntax','vocabulary','phraseology','grammar','conventions'],axis=1)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.20)\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFyKJfx4x2dB",
    "outputId": "367e8b83-84d7-449b-dcb9-6bd275645a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 200) (783, 200)\n"
     ]
    }
   ],
   "source": [
    "#padding to make all the vectors of the same length\n",
    "\n",
    "def pad_text(text,tokenizer,max_len):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(text),maxlen=max_len,padding='post')\n",
    "\n",
    "\n",
    "def text_padding(train,test,max_len):\n",
    "    vocab=5000\n",
    "    token=Tokenizer()\n",
    "    token.fit_on_texts(train)\n",
    "    padded_train_text=pad_text(train,token,max_len)\n",
    "    padded_test_text=pad_text(test,token,max_len)\n",
    "    return padded_train_text,padded_test_text,token\n",
    "comm_len=200\n",
    "train_com_pad,test_com_pad,token_com= text_padding(X_train['full_text'],X_test['full_text'],comm_len)\n",
    "\n",
    "print(train_com_pad.shape,test_com_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eraKdj-rx-W_"
   },
   "outputs": [],
   "source": [
    "def generate_embedding_matrix(token):\n",
    "    embedding_path='./crawl-300d-2M.vec' #pre trained FastText English word vectors released by FB\n",
    "    embedding_size=300\n",
    "    vocab_size=5000\n",
    "    embedding_index={}\n",
    "    with open(embedding_path, 'r',encoding=\"utf8\") as f:\n",
    "         for line in f:\n",
    "                values = line.rstrip().rsplit(' ')\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embedding_index[word] = coefs\n",
    "    num_words = len(token.word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "    for word, i in token.word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the embedding matrix or use the already stored one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNBuWe-gyR1d",
    "outputId": "277609e5-75b0-4584-a2d0-8a589935e169"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\acads\\Y2S2\\CS3264\\project\\idea 1\\erdos-hickory-22\\LSTM.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# generating the embedding matrix containing (if not already generated)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embedding_comm \u001b[39m=\u001b[39m generate_embedding_matrix(token_com)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(embedding_comm\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\acads\\Y2S2\\CS3264\\project\\idea 1\\erdos-hickory-22\\LSTM.ipynb Cell 11\u001b[0m in \u001b[0;36mgenerate_embedding_matrix\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m embedding_index\u001b[39m=\u001b[39m{}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(embedding_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m,encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m      \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             values \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip()\u001b[39m.\u001b[39mrsplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             word \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generating the embedding matrix containing (if not already generated)\n",
    "embedding_comm = generate_embedding_matrix(token_com)\n",
    "print(embedding_comm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the embedded matrix for trained\n",
    "np.savetxt('./data/Embedded_matrix_trained_data.csv',embedding_comm,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the already stored embedded matrix for the trained data\n",
    "embedding_comm = np.loadtxt('./data/Embedded_matrix_trained_data.csv', delimiter=',')\n",
    "#print(embedding_comm1.shape)\n",
    "#print(embedding_comm1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqnevNmx56pZ"
   },
   "outputs": [],
   "source": [
    "#reshaping the data \n",
    "X_train=[train_com_pad,train_com_pad]\n",
    "X_test=[test_com_pad,test_com_pad]\n",
    "y_train=np.array(2.0*y_train,dtype=np.float64)\n",
    "y_test=np.array(2.0*y_test, dtype=np.float64)\n",
    "\n",
    "# mean columwise rmse\n",
    "def mcrmse(y_true, y_pred):\n",
    "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n",
    "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqrDfRu76DkI"
   },
   "outputs": [],
   "source": [
    "# We will use the Adam optimizer \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "def LSTM_CNN1D(comm_len,token_com):\n",
    "    drop_lstm = 0.25\n",
    "    drop_dense = 0.25\n",
    "    num_lstm=150\n",
    "    input_1 = Input(shape=(comm_len,),name = 'input_comment_1')\n",
    "    embedding_layer_1 = Embedding(len(token_com.word_index) + 1,300,weights=[embedding_comm],input_length=comm_len,trainable=False,dtype=tf.float32)(input_1)\n",
    "    conv_1_1 = Conv1D(64,3,strides=1, padding='same',activation='relu')(embedding_layer_1)\n",
    "    lstm_1_1 = LSTM(64,dropout=drop_lstm,return_sequences=True,dtype=tf.float32)(embedding_layer_1)\n",
    "    concate_1 = keras.layers.Concatenate(axis=-1)([conv_1_1, lstm_1_1])\n",
    "    flatten_1 = Flatten()(concate_1)\n",
    "\n",
    "    # creating layers for parent comment data\n",
    "    input_2 = Input(shape=(comm_len,),name = 'input_comment_2')\n",
    "    embedding_layer_2 = Embedding(len(token_com.word_index) + 1,300,weights=[embedding_comm],input_length=comm_len,trainable=False,dtype=tf.float32)(input_2)\n",
    "    conv_1_1 = Conv1D(128,3,strides=1, padding='same',activation='relu')(embedding_layer_2)\n",
    "    lstm_1_2 =LSTM(128,dropout=drop_lstm,return_sequences=True,dtype=tf.float32)(embedding_layer_2)\n",
    "    concate_2 = keras.layers.Concatenate(axis=-1)([conv_1_1, lstm_1_2])\n",
    "    flatten_2 = Flatten()(concate_2)\n",
    "\n",
    "    \n",
    "    concatenated_layer = keras.layers.concatenate([flatten_1,flatten_2],axis=-1)\n",
    "\n",
    "    # creating further layers\n",
    "    x = Dense(128, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(concatenated_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(16, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(x)\n",
    "    output = Dense(6,activation='linear')(x)\n",
    "    model = Model(inputs = [input_1,input_2], outputs = [output])\n",
    "    model.compile(optimizer=adam, loss = mcrmse, metrics = mcrmse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ec8AGYpG6SYm",
    "outputId": "3cac99b7-601c-4e9d-cc4c-d6b743e5588a"
   },
   "outputs": [],
   "source": [
    "# Loading the saved model\n",
    "model=LSTM_CNN1D(comm_len,token_com)\n",
    "model.load_weights(\"./data/LSTM_weights_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jeLdKTq6rD1"
   },
   "outputs": [],
   "source": [
    "#reduce_lr reduces the learning rate when the metric has stoppes improving for 2 epochs. \n",
    "#Using EarlyStopping to stop the calculation upon reaching enough accuracy\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_mcrmse', factor = 0.25, patience = 2, verbose = 1)\n",
    "earlystop = EarlyStopping(monitor = 'val_mcrmse',  mode=\"min\",min_delta = 0.01, patience = 5,verbose = 1)\n",
    "callbacks = [reduce_lr,earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crrtOMqr6xJS",
    "outputId": "a7e2087b-140f-4f2e-9b46-a3542eb73ec6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\acads\\Y2S2\\CS3264\\project\\idea 1\\erdos-hickory-22\\LSTM.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prave/Desktop/acads/Y2S2/CS3264/project/idea%201/erdos-hickory-22/LSTM.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hitory1\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mfit(x\u001b[39m=\u001b[39mX_train,y\u001b[39m=\u001b[39my_train,epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,validation_data\u001b[39m=\u001b[39m(X_test, y_test),callbacks\u001b[39m=\u001b[39mcallbacks)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "hitory1=model.fit(x=X_train,y=y_train,epochs=30,batch_size=32,validation_data=(X_test, y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model and weights for future \n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"./data/LSTM_model_final.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"./data/LSTM_weights_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved model\n",
    "json_file=open('./data/LSTM_model_final.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"./data/LSTM_weights_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 6s 239ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicted y\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbA64eigmjAM",
    "outputId": "edd7d61e-83fe-41af-ed0e-5f67b163c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error in cohesion is 1.6313045548918779\n",
      "mean squared error in syntax is 1.5434194716678813\n",
      "mean squared error in vocabulary is 1.2058599809220685\n",
      "mean squared error in phraseology is 1.604282528726325\n",
      "mean squared error in grammar is 1.7494199239603103\n",
      "mean squared error in conventions is 1.506549082802304\n"
     ]
    }
   ],
   "source": [
    "# calculating mean squared errors\n",
    "categories=training_data.columns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "for i in range(6):\n",
    "    error = mean_squared_error(y_test[:,i],y_pred[:,i])\n",
    "    print(\"mean squared error in\",categories[i+2],\"is\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
